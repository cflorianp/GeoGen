{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kafkaReceiveDataPy\n",
    "This notebook receives data from Kafka on the topic 'test', and stores it in the 'time_test' table of Cassandra (created during the building of the Docker container, see cassandra_init.script).\n",
    "\n",
    "```\n",
    "CREATE KEYSPACE test_time WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1};\n",
    "\n",
    "CREATE TABLE test_time.sent_received(\n",
    " time_sent TEXT,\n",
    " time_received TEXT,\n",
    "PRIMARY KEY (time_sent)\n",
    ");\n",
    "```\n",
    "\n",
    "A message that gives the current time is received every second. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--conf spark.ui.port=4040 --packages org.apache.spark:spark-streaming-kafka_2.11:1.6.1,com.datastax.spark:spark-cassandra-connector_2.11:1.6.0-M2 pyspark-shell'\n",
    "import time\n",
    "import json\n",
    "import datetime\n",
    "import sys\n",
    "import shutil\n",
    "from math import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load modules and start SparkContext\n",
    "Note that SparkContext must be started to effectively load the package dependencies. Two cores are used, since one is need for running the Kafka receiver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, Row\n",
    "conf = SparkConf() \\\n",
    "    .setAppName(\"Streaming test\") \\\n",
    "    .setMaster(\"local[2]\") \\\n",
    "    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "sc = SparkContext(conf=conf) \n",
    "sqlContext=SQLContext(sc)\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SaveToCassandra function\n",
    "Takes a list of tuple (rows) and save to Cassandra "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def saveToCassandra(rows):\n",
    "    if not rows.isEmpty(): \n",
    "        sqlContext.createDataFrame(rows).write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "        .mode('append')\\\n",
    "        .options(table=\"test_geogen\", keyspace=\"test_geogen_ks\")\\\n",
    "        .save()\n",
    "        \n",
    "def distance_from_coord(lat1, lon1, lat2, lon2):\n",
    "    D2R = pi / 180\n",
    "    dlat = (lat1 - lat2) * D2R\n",
    "    dlon = (lon1 - lon2) * D2R\n",
    "    a = pow(sin(dlat/2.0), 2) + cos(lat1*D2R) * cos(lat2*D2R) * pow(sin(dlon/2.0), 2)\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1-a))\n",
    "    d = 6367 * c\n",
    "    return d\n",
    "        \n",
    "def updateState(newValues, current_state):\n",
    "    if current_state is None:\n",
    "        try:\n",
    "            current_state = {\"total\":0, \"latitude\":newValues[0][0], \"longitude\":newValues[0][1]}\n",
    "        except:\n",
    "            return None\n",
    "    try:\n",
    "        for row in newValues:\n",
    "            current_state['total'] = current_state['total'] + distance_from_coord(current_state['latitude'], current_state['longitude'], row[0], row[1])\n",
    "            current_state['latitude'] =  row[0]\n",
    "            current_state['longitude'] = row[1]\n",
    "    except Exception as e:\n",
    "        current_state['error'] = e\n",
    "    return current_state\n",
    "    \n",
    "def testUpdater(newValues, current):\n",
    "    if current is None:\n",
    "        print(\"creating\")\n",
    "        current = 1\n",
    "    print(newValues)\n",
    "    return current\n",
    "\n",
    "def isUpdaterCalled(newValues, current):\n",
    "    print(\"updater called\")\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Create streaming task\n",
    "* Receive data from Kafka 'test' topic every five seconds\n",
    "* Get stream content, and add receiving time to each message\n",
    "* Save each RDD in the DStream to Cassandra. Also print on screen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def createContext():\n",
    "    print(\"context created\")\n",
    "    #initialStateRDD = sc.parallelize([])\n",
    "    #shutil.rmtree('checkpoint')\n",
    "\n",
    "\n",
    "    ssc = StreamingContext(sc, 5)\n",
    "    kvs = KafkaUtils.createStream(ssc, \"127.0.0.1:2181\", \"spark-streaming-consumer\", {'geoData': 1})\n",
    "    data = kvs.map(lambda x: json.loads(x[1]))\n",
    "    \"\"\"rows= data.map(lambda x:Row( \\\n",
    "                                msisdn=x['msisdn'], \\\n",
    "                                time_sent=datetime.datetime.fromtimestamp(x['timestamp']).strftime(\"%Y-%m-%d %H:%M:%S\"), \\\n",
    "                                latitude = x['latitude'], \\\n",
    "                                longitude = x['longitude'], \\\n",
    "                                radius = x['radius'] \\\n",
    "                               ))#\"\"\"\n",
    "    mapped = data.map(lambda x:(x['msisdn'],(x['latitude'],x['longitude'])))\n",
    "    #rows.foreachRDD(saveToCassandra)\n",
    "    status = mapped.updateStateByKey(updateState)\n",
    "    status.pprint()\n",
    "    #mapped.pprint()\n",
    "    #rows.pprint()\n",
    "    return ssc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "context created\n",
      "-------------------------------------------\n",
      "Time: 2017-01-12 09:16:55\n",
      "-------------------------------------------\n",
      "(8, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "(2, {'latitude': 45.77818, 'total': 0.0, 'longitude': 4.85675})\n",
      "(4, {'latitude': 45.77818, 'total': 0.0, 'longitude': 4.85675})\n",
      "(6, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "(1, {'latitude': 45.77818, 'total': 0.0, 'longitude': 4.85675})\n",
      "(3, {'latitude': 45.77818, 'total': 0.0, 'longitude': 4.85675})\n",
      "(9, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "(5, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "(7, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-01-12 09:17:00\n",
      "-------------------------------------------\n",
      "(8, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "(2, {'latitude': 45.77818, 'total': 0.0, 'longitude': 4.85675})\n",
      "(4, {'latitude': 45.77818, 'total': 0.0, 'longitude': 4.85675})\n",
      "(6, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "(1, {'latitude': 45.77985200000004, 'total': 0.6214863495527894, 'longitude': 4.864402249999998})\n",
      "(3, {'latitude': 45.77818, 'total': 0.0, 'longitude': 4.85675})\n",
      "(9, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "(5, {'latitude': 45.77818, 'total': 0.6541956872931507, 'longitude': 4.85675})\n",
      "(7, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-01-12 09:17:05\n",
      "-------------------------------------------\n",
      "(8, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "(2, {'latitude': 45.77985200000004, 'total': 0.6214863495527894, 'longitude': 4.864402249999998})\n",
      "(4, {'latitude': 45.77818, 'total': 0.0, 'longitude': 4.85675})\n",
      "(6, {'latitude': 45.77818, 'total': 0.6541956872931507, 'longitude': 4.85675})\n",
      "(1, {'latitude': 45.77985200000004, 'total': 0.6214863495527894, 'longitude': 4.864402249999998})\n",
      "(3, {'latitude': 45.77985200000004, 'total': 0.6214863495527894, 'longitude': 4.864402249999998})\n",
      "(9, {'latitude': 45.77994000000004, 'total': 0.0, 'longitude': 4.864804999999998})\n",
      "(5, {'latitude': 45.77818, 'total': 0.6541956872931507, 'longitude': 4.85675})\n",
      "(7, {'latitude': 45.77818, 'total': 0.6541956872931507, 'longitude': 4.85675})\n",
      "\n",
      "-------------------------------------------\n",
      "Time: 2017-01-12 09:17:10\n",
      "-------------------------------------------\n",
      "(8, {'latitude': 45.77818, 'total': 0.6541956872931507, 'longitude': 4.85675})\n",
      "(2, {'latitude': 45.77985200000004, 'total': 0.6214863495527894, 'longitude': 4.864402249999998})\n",
      "(4, {'latitude': 45.77985200000004, 'total': 0.6214863495527894, 'longitude': 4.864402249999998})\n",
      "(6, {'latitude': 45.77818, 'total': 0.6541956872931507, 'longitude': 4.85675})\n",
      "(1, {'latitude': 45.77985200000004, 'total': 0.6214863495527894, 'longitude': 4.864402249999998})\n",
      "(3, {'latitude': 45.77985200000004, 'total': 0.6214863495527894, 'longitude': 4.864402249999998})\n",
      "(9, {'latitude': 45.77818, 'total': 0.6541956872931507, 'longitude': 4.85675})\n",
      "(5, {'latitude': 45.77818, 'total': 0.6541956872931507, 'longitude': 4.85675})\n",
      "(7, {'latitude': 45.77818, 'total': 0.6541956872931507, 'longitude': 4.85675})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ssc = StreamingContext.getOrCreate(\"check15\",createContext)\n",
    "ssc.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ssc.stop(stopSparkContext=False,stopGraceFully=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Cassandra table content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=sqlContext.read\\\n",
    "    .format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .options(table=\"test_geogen\", keyspace=\"test_geogen_ks\")\\\n",
    "    .load()\n",
    "data.show()\n",
    "sys.getsizeof(data)\n",
    "rw = data.filter(data.msisdn == 1).filter(data.time_sent > datetime.datetime(2017, 1, 11,14,15,30)).filter(data.latitude > 45.78).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Get Cassandra table content using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.registerTempTable(\"sent_received\");\n",
    "data.printSchema()\n",
    "data=sqlContext.sql(\"select * from sent_received\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
